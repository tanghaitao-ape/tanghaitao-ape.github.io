{"posts":[{"title":"ping github.com 失败解决方案","content":"问题解决 使用 ping 检测工具找一个能够 ping 通 github.com 的 IP，检测工具网址如下： http://ping.chinaz.com/github.com 选择一个响应较快的 IP，添加到 C:\\Windows\\System32\\drivers\\etc\\host 文件末尾： 140.82.113.3 github.com 再次检测远程连接，成功。 ","link":"https://tanghaitao-ape.github.io/post/ping-githubcom-shi-bai-jie-jue-fang-an/"},{"title":"基于LC-BLSTM在流式语音识别的应用","content":" 论文链接 问题 语音流式识别(chunk窗)： LSTM效果较差，无右视野 BLSTM效果较好，右视野为整句话 初代：CSC-LSTM，𝑁𝑙+𝑁𝑐+𝑁𝑟𝑁_𝑙 + 𝑁_𝑐+ 𝑁_𝑟Nl​+Nc​+Nr​，相比BLSTM在𝑁𝑙𝑁_𝑙Nl​和𝑁𝑟𝑁_𝑟Nr​部分存在更多计算量 二代：LC-BLSTM， 𝑁𝑐+𝑁𝑟𝑁_𝑐+ 𝑁_𝑟Nc​+Nr​，取消𝑁𝑙𝑁_𝑙Nl​计算量，但是存在𝑁𝑟𝑁_𝑟Nr​ 的计算量。具体详见图1 分析𝑁𝑟𝑁_𝑟Nr​部分发现： 提供𝑁𝑐𝑁_𝑐Nc​前向传播中反向计算的hidden state 第iii层𝑁𝑐𝑁_𝑐Nc​前向传播计算结果为第i+1i+1i+1层提供一部分输入 图1： 音频分为多个chunk 方法 具体详见图2 前向计算 #1~#𝑁_𝑐帧输入到模型中，并将其输出传给下一层 第#𝑁_𝑐帧也会作为下一个chunk 第#1帧 hidden state的初始化 #(N_𝑐+1)~#(𝑁_𝑐+𝑁_𝑟)输入到模型中，结果作为下一层的部分输入 反向计算 初始状态为000，从#(𝑁_𝑐+𝑁_𝑟)~#(𝑁_𝑐+1)输入到模型中，将结果进行输出 利用第#(𝑁_𝑐+1)的hidden state作为#𝑁_𝑐的hidden state初始化 #𝑁_𝑐~#1帧输入到模型中，并将其输出传给下一层 汇总 将前向和反向计算#1~#𝑁_𝑐和#(𝑁_𝑐+1)~#(𝑁_𝑐+𝑁_𝑟)输出进行拼接，然后将前向、反向拼接的结果整合，再作为下一层的输入 当网络层全部计算完，只输出#1~#𝑁_𝑐的结果 图2：LC-BLSTM结构图 基于DNN近似的改进算法 图3：基于DNN近似的LCBLSTM 图4：Pytorch LSTM 接口说明 具体细节说明： 反向传递时，NcN_cNc​帧的hidden state初始化由NrN_rNr​过DNN输出决定 LSTM在pytorch的格式如图4所示。num_layer * num_directions 在每一层的时候结果为1；batch为NrN_rNr​的batch，这里和NcN_cNc​相等；hidden_size需要保证DNN的输出维度和LSTM的映射矩阵维度相等 DNN输出的维度为batch * seq * hidden_size，只需要将seq求平均，从而保证可以将结果作为NcN_cNc​部分LSTM反向传播状态初始化的值 需要注意：只是初始化h0，c0表示LSTM阀值的控制程度，其结果依然用000进行初始化 基于RNN近似的改进算法 图5：基于RNN近似的LCBLSTM 具体细节和DNN相似，不在赘述 具体实现 LC-BLSTM伪代码 输入：特征矩阵xs(batch*length*dim)，Nc，Nr，layers 输出：高层表示ys(batch*length_*dim_) for i in range(0, length, Nc): xs_chunk.append(xs[:, i : i + Nc + Nr]) h0_forward, c0_forward = zeros(1, batch, hidden_size), zeros(1, batch, hidden_size) h0_backward, c0_backward = zeros(1, batch, hidden_size), zeros(1, batch, hidden_size) for i in range(length(xs_chunk)): xs_ = xs_chunk[i] for k in range(layers): xs_forward, h0_forward, c0_forward = lstm_forward[k](xs_, h0_forward, c0_forward) xs_backward, h0_backward, c0_backward = lstm_backward[k](xs_[::-1], h0_backward, c0_backward) xs_ = projection_layer[k](cat(xs_forward, xs_backward[::-1], 1)) ys.append(xs_[:, 0 : Nc]) return ys 基于DNN近似的LC-BLSTM伪代码 输入：特征矩阵xs(batch*length*dim)，Nc，Nr，layers 输出：高层表示ys(batch*length_*dim_) for i in range(0, length, Nc): xs_chunk.append(xs[:, i : i + Nc + Nr]) h0_forward, c0_forward, c0_backward = zeros(1, batch, hidden_size), zeros(1, batch, hidden_size), zeros(1, batch, hidden_size) for i in range(length(xs_chunk)): xs_ = xs_chunk[i] for k in range(layers): xs_forward, h0_forward, c0_forward = lstm_forward[k](xs_[0 : Nc], h0_forward, c0_forward) xs_Nc_Nr = DNN[k](xs_[Nc : Nr]) # dim  hidden_size h0_backward = mean(xs_backward, 1).transpose(0,1) # batch, 1, hidden_size --&gt; 1, batch, hidden_size xs_backward, h0_backward, c0_backward = lstm_backward[k](xs_[0 : Nc], h0_backward, c0_backward) xs_forward = cat(xs_forward, xs_Nc_Nr, 1); xs_backward = cat(xs_backward, xs_Nc_Nr, 1) xs_ = projection_layer[k](cat(xs_forward, xs_backward, 1)) ys.append(xs_[:, 0 : Nc]) return ys 实验 数据集：320-hr Switchboard 表1：右视野WER对比结果 表2：FABDI不同连接方式对比 表3：FABSR节点和target delay对比结果 ","link":"https://tanghaitao-ape.github.io/post/ji-yu-lc-blstm-zai-liu-shi-yu-yin-shi-bie-de-ying-yong/"}]}